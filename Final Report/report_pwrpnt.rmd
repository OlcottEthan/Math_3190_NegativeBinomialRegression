---
title: "Negative Binomial Regression"
author: "Samuel Peterson, Elijah Wagner, and Ethan Olcott"
date: "April 17, 2024"
output: 
 beamer_presentation:
    theme: CambridgeUS
header-includes:
  - \usepackage{amsmath}
  - \makeatletter
  - \preto{\@verbatim}{\topsep=-10pt \partopsep=4pt }
  - \makeatother
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
urlcolor: blue
---

\tableofcontents


```{r setup, include = F}
library(NegativeBinomialRegression)
library(tidyverse)
library(lubridate)
library(MASS)
library(AER)
```

# Negative Binomial Distribution
## The Binomial Distribution

The Binomial Distribution is a probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, where there is only success or failure as outcomes.

Some key assumptions and characteristics are:

1. Fixed Number of Trials
2. Independent Trials
3. Boolean Outcomes
4. Constant Probability
5. Discrete Distribution

## Connection to Negative Binomial Distribution

The Binomial Distribution predicts the number of successes when the number of trials is fixed. The Negative Binomial Distribution differs from this in that the number of successes is fixed, and the number of required trials is random.

# The Negative Binomial

## Necessary Conditions

1. Independent Trials
2. Boolean Outcomes
3. Constant Probability
4. Trials Continue Till Desired Success Total Reached

The Negative Binomial Distribution has many of the same types of conditions as the Binomial Distribution (as to be expected), but the major difference comes in the fixed variable. The Negative Binomial is geared towards identifying the number of trials needed for a certain amount of trials.

## Math Behind Negative Binomial

The Probability Mass Function of the Negative Binomial Distribution with r = number of successes, p = probability of success, and k = the number of failures before the $r$th success is given by:

$\text{NegBin}(k\mid r, p) = \binom{k + r - 1}{r - 1} \cdot p^{r} \cdot (1 - p)^k\qquad\text{for}\qquad k,r=1, 2,\cdots\quad\text{and}\quad0<p\leq1$

The Expected Value and Variance of the Negative Binomial Distribution described above are as follows:

**Expected Value**: $\quad E(\mathbf X)=\displaystyle\frac{r\cdot(1 - p)}{p}$

**Variance**: $\quad V(\mathbf X)=\displaystyle\frac{r\cdot(1 - p)}{p^2}$

*Note: There are many variations of the Negative Binomial Distribution when using separate parameters. There are variations when you can count the number of trials, redefine the r as the number of failures, or other variations of them. These do change the PMF and $E(\mathbf X)$ and $V(\mathbf X)$ functions for the different variations.*

## Statistical Inference

Minimum Variance Unbiased Estimator for p:

If the probability of success isn't known and sampling will continue until the $r$th success is found, a sufficient statistic for the experiment then is $k$, the number of failures. In this case to estimate $p$, the minimum variance unbiased estimator is:

$$\hat{p} = \frac{r - 1}{r + k - 1}$$

Maximum Likelihood Estimate of $p$:

$$\hat{p} = \frac{r}{r + k}$$

This estimate is biased, however its inverse $\frac{r + k}{r}$ is an unbiased estimate of $\frac{1}{p}$

## Connection to the Poisson Distribution

The Poisson Distribution and the Negative Binomial Distribution are both distributions that can be seen as similar in nature. It is possible to look at the Negative Binomial Distribution as the generalization of the Poisson Distribution. This can be done because the Negative Binomial Distribution converges to the Poisson Distribution as the number of trials goes to infinity and the probability of success is relatively small. The math of this is as follows:

\begin{align*}
\lim_{r\to\infty}\text{NegBin}\Big(r,\frac{r}{r+\lambda}\Big)
&=\lim_{r\to\infty}\left(\frac{(k+r)!}{k!r!}\left(1-\frac{r}{r+\lambda}\right)^k\left(\frac{r}{r+\lambda}\right)^r\right)\\
&=\lim_{r\to\infty}\left(\frac{\lambda^k}{k!}\cdot\frac{(r+k)!}{r!(r+\lambda)^k}\cdot\frac{1}{\left(1+\frac{\lambda}{r}\right)^r}\right)\\
&=\frac{\lambda^k}{k!}\cdot1\cdot\frac{1}{e^\lambda}\\
&=\frac{\lambda^ke^{-\lambda}}{k!}\\
&=\text{Poisson}(\lambda)\qquad\qquad\blacksquare
\end{align*}

<br>
Thus when $p=\dfrac{r}{r+\lambda}\Rightarrow\lambda=\dfrac{(1-p)r}p$ and $r\to\infty$ we have $\text{NegBin}(r,p)=\text{Poisson}(\lambda)$.

The Negative Binomial Distribution allows for the Variance and Mean to vary (which the Poisson Distribution does not allow for) and thus can deal with some data that the Poisson cannot. This is especially useful when the Poisson Distribution has issues with data that is over-dispersed and provides an alternative to the Quasi-Poisson option we explored in this class.


## Special Parametrization

Now let $\mu=\frac{r(1-p)}p$, $\alpha=\frac1r$, and $y=k$. Then see that we have,

$$\text{NegBin}(y\mid\mu,\alpha)=\left(\begin{matrix}y+\frac1\alpha-1\\ \frac1\alpha-1\end{matrix}\right)\left(\frac1{1+\alpha\mu}\right)^{\frac1\alpha}\left(\frac{\alpha\mu}{1+\alpha\mu}\right)^y$$

and,

$$E(\mathbf X)=\mu\qquad\text{and}\qquad V(\mathbf X)=\mu\left(1+\frac{\mu}{\alpha}\right)$$

This parametrization is especially helpful for finding the log likelihood function as it contains the mean and a value related to $r$, thus allowing us to simplify further.

# Negative Binomial Regression
## Overview

Negative Binomial regression is very similar to Poisson regression and is applied when the data is a discrete, quantitative response variable ranging from 0 to infinity and larger counts are rare. The difference comes in with the dispersion. Since the Negative Binomial distribution is an extension of the Poisson distribution with the extra parameter $r$ the variance of the data does not have to be the same as the mean or expected value.

We therefore have the same link function for both Poisson and Negative Binomial regression, $\ln(\widehat{E[Y\mid\mathrm x_i]})=\mathrm x_i\beta_i$ where $\mathrm x_i$ is the $i$th row of the model matrix $\mathbf X$.

Or equivalently, since $E(Y)=\frac{r\ \cdot\ (1-p)}p=\mu$, we have $\ln\left(\widehat{\mu\mid \mathrm x_i}\right)=\beta_0+\beta_1x_i+\cdots+\beta_nx_n=\mathbf X\boldsymbol\beta$ where $x_i$ is the $i$th variable.

Therefore we have

$$\widehat{\mu\mid\mathrm x_i}=e^{\mathrm x_i\cdot\boldsymbol\beta}$$

Thus our regression equation is given by,

$$p(y_i)=\left(\begin{matrix}y_i+1/\alpha-1\\1/\alpha-1\end{matrix}\right)\left(\frac{1}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\right)^{1/\alpha}\left(\frac{\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\right)^{y_i}$$

## Negative Binomial Regression Assumptions

1. **Negative Binomial Response**: The response variable is conditionally a count unit of time or space, described by the Negative Binomial Distribution. (Remember that the Poisson is a special limiting Negative Binomial result)
2. **Independence**: The observations are independent of one another.
3. **Linearity**: The log of the mean rate must be a linear function of $\mathrm x$.
4. **over-dispersed**: The variance is larger than the mean giving and overdispersion of the data.

Therefore we can write the distributions as,

$$Y\mid\beta_0,\beta_1,\cdots,\beta_p,r\;\overset{\text{ind}}\sim\;\text{NegBin}(\mu,r)
\quad\text{with}\quad
\ln(\mu_i)=\beta_0+\beta_1x_1+\cdots+\beta_px_p$$

with priors:
\begin{align*}
\beta_0\;&\sim\;N(m_0,s_0^2)\\
\beta_1\;&\sim\;N(m_1,s_1^2)\\
&\vdots\\
\beta_p\;&\sim\;N(m_p,s_p^2)\\
r\;&\sim\;\text{Exp}(\cdots)
\end{align*}

where $m_i$ and $s_i$ are the mean and standard deviation for each $x_i$ variable.

\clearpage
## Parameter Estimation

The parameters $r, \beta_0, \beta_1, \cdots, \beta_p$ are also estimated using **maximum likelihood estimation** similar to Poisson regression. See that by using the special parametrization we have the following,

\begin{align*}
L(\alpha,\beta)
&=\prod_{i=1}^np(y_i)\\
&=\prod_{i=1}^n\left(\begin{matrix}y_i+1/\alpha-1\\1/\alpha-1\end{matrix}\right)\left(\frac1{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\right)^{1/\alpha}\left(\frac{\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\right)^{y_i}\\
&=\prod_{i=1}^n\frac{(y_i+1/\alpha-1)!}{(y_i)!(1/\alpha-1)!}\left(\frac1{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\right)^{1/\alpha}\left(\frac{\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\right)^{y_i}\\
\end{align*}

Thus we have log likelihood function,

\begin{align*}
    \ell(\alpha,\boldsymbol\beta)
    &=\ln\left(L(\alpha,\boldsymbol\beta)\right)\\
    &=\ln\left(\prod_{i=1}^n\frac{(y_i+1/\alpha-1)!}{(y_i)!(1/\alpha-1)!}\left(\frac1{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\right)^{1/\alpha}\left(\frac{\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\right)^{y_i}\right)\\
    &=\sum_{i=1}^n\Big[\ln\left((y_1+1/\alpha-1)!\right)-\ln\left(y_i!\right)-\ln\left((1/\alpha-1)!\right)+\\
    &\qquad\qquad\qquad\qquad y_i\ln(\alpha)+y_i\mathrm x_i\cdot\boldsymbol\beta-(y_i+1/\alpha)\ln\left(1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}\right)\Big]
\end{align*}

Then clearly the values of $\alpha$ and $\boldsymbol\beta$ that maximize $\ell(\alpha,\boldsymbol\beta)$ will be the maximum likelihood estimates and that we can solve for $\alpha$ and $\boldsymbol\beta$ by a standard derivative test, by use of the Hessian matrix, or by using a gradient descent method.

See that if we let $\psi^{(m)}:= \frac{d^m}{dz^m}\psi(z)=\frac{d^{m+1}}{dz^{m+1}}\ln\Gamma(z)$ be the polygamma function of order $m$ then we can write the partial derivatives as:

\vspace{-2ex}\begin{align*}
\frac{\partial\ell(\alpha,\boldsymbol\beta)}{\partial\alpha}
&=\frac{\psi^{(0)}(1/\alpha)+\ln(1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta})+y_i\alpha-\psi^{(0)}(y_i+1/\alpha)}{\alpha^2}-\frac{(y_i+1/\alpha)e^{\mathrm x_i\cdot\boldsymbol\beta}}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\\
\frac{\partial\ell(\alpha,\boldsymbol\beta)}{\partial\beta_0}
&=y_i-\frac{\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}(y_i+1/\alpha)}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\\
\frac{\partial\ell(\alpha,\boldsymbol\beta)}{\partial\beta_1}
&=y_i x_1-\frac{\alpha x_1 e^{\mathrm x_i\cdot\boldsymbol\beta}(y_i+1/\alpha)}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}\\
&\ \vdots\\
\frac{\partial\ell(\alpha,\boldsymbol\beta)}{\partial\beta_p}
&=y_i x_p-\frac{\alpha x_p e^{\mathrm x_i\cdot\boldsymbol\beta}(y_i+1/\alpha)}{1+\alpha e^{\mathrm x_i\cdot\boldsymbol\beta}}
\end{align*}

Let us run a model in R. We can use `glm.nb` in the `MASS` package to run a negative binomial. We will use `bike_rentals` in the `NegativeBinomialRegression` package available on github.com from \newline "OlcottEthan/NegativeBinomialRegression".

```{r}
bikes_bridges_nb <- glm.nb(total ~ temp_high, data = bikes_bridges)
```

\clearpage

```{r}
ggplot(bikes_bridges, aes(y = total, x = temp_high)) +
  geom_point() +
  labs(title = "Bikes Bridges Plot")
```


## Residuals
Just as in Poisson regression we can use Pearson residual in Negative Binomial regression where each residual $\hat\epsilon_i^p$ is given by,

\vspace{-2ex}
$$\hat\epsilon_i^{\mathrm p}=\frac{y_i-\hat\mu_i}{\sqrt{\hat\mu_i+\alpha\hat\mu_i^2}}$$
\vspace{-1ex}
Another common option is the Anscombe residual given by 

$$\hat\epsilon_i^{\mathrm a}=\frac{\frac3\alpha\Big[(1+\alpha y_i)^{2/3}-(1+\alpha\hat\mu_i)^{2/3}\Big]+3\left(y_i^{2/3}-\hat\mu_i^{2/3}\right)}{2\left(\hat\mu_i+\alpha\hat\mu_i^2\right)^{1/6}}$$
\vspace{-1ex}
Both of these residuals are defined in order to prioritize a normal distribution.

We can get the Pearson residuals using the `residual` function,
```{r}
bikes_bridges_p_resids <- residuals(bikes_bridges_nb, type = 'pearson')
```

\vspace{-1ex}and the predicted values using the `predict` function.
```{r}
bikes_bridges_preds <- predict(bikes_bridges_nb, type = 'link')
```



## Regression Diagnostics

There are a couple of helpful diagnostics for Negative Binomial regression that use both hypothesis tests and plots, For the most part these are the same as the tests given in Poisson regression because many of the assumptions are the same.

1. A residual plot of the Pearson residuals vs the predicted values. There should be a random spread of the residuals
2. A QQ-plot of the Pearson or Anscombe residuals. These should appear to be normally distributed.
3. A deviance goodness-of-fit (GOF) test.

```{r}
bikes_bridges_resids <- data.frame(
  residuals = bikes_bridges_p_resids, 
  predictors = bikes_bridges_preds)
```

```{r}
ggplot(bikes_bridges_resids, aes(x=predictors, y = residuals)) +
  geom_point() +
  labs(x="Linear Predictors",
       y = "Pearson Residuals",
       title = "Residual Plot")
```

This plot is fairly well dispersed and appears to be fairly random.

```{r}
ggplot(bikes_bridges_resids, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(x = "Theoretical Quantiles",
       y = "Sample Quantiles",
       title = "QQ Plot: Pearson Residuals")
```

The QQ-plot appears to be fairly normal although it does have some issues on the ends indicating that there could be issues with hetoroskedacicity

Now we can perform a goodness of fit test.

```{r}
1 - pchisq(bikes_bridges_nb$deviance, df = bikes_bridges_nb$df.residual)
```

Since the $p$_value is 0.380 we fail to reject the Null hypothesis and can trust that the model is a good representation of the data.


\clearpage
## Confidence Intervals for Coefficients

In order to get the confidence intervals for the coefficients, *intercept* and *slope*, we can profile the likelihood function. This is a process of replacing the other parameters that we are not interested in with their maximum-likelihood estimates and then getting the intervals desired.


In R we can do this by exponentiating the `confint` function.

```{r}
exp(confint(bikes_bridges_nb))[2,]
```

We can also plot the confidence intervals for the mean just as we did with Poisson regression just being careful to reference the correct function of `glm.nb` instead of `glm`.

```{r}
ggplot(bikes_bridges, aes(y = total, x = temp_high)) +
  geom_point() +
  geom_smooth(method = "glm.nb",
              formula = "y ~ x", se = T) +
  labs(title = "Bikes Bridges Plot with CI")
```

\clearpage
## Coefficient interpretation

We interpret the coefficients just the same as we do in Poisson regression since our model is the same.

**Intercept**: The predicted mean of the Poisson distribution when each $x_i=0$ is $\exp(\hat\beta_0)$.

**Slope**: The predicted change in the mean of the Poisson distribution when $x_i$ increases by one is $\exp(\hat\beta_i)$.

```{r}
bikes_bridges_nb$coefficients
exp(bikes_bridges_nb$coefficients)
```

Thus the predicted mean of total number of bikes that would have crossed the four bridges in Manhattan in a day with the high temperature being zero is $e^{8.612}=5497$.

The predicted change in the mean of the total number of bikes crossing every bridge in Manhattan when the high temperature increases by one is $e^{0.0206}=1.016$ 


\clearpage
# Another example

Let's consider anther data set! 

```{r}
bike <- bike_rentals %>%
  dplyr::select(yr, mnth, holiday, workingday, temp, hum, windspeed, cnt)
bike_nb <- glm.nb(cnt ~ ., data = bike)

bike_nb_df <- data.frame(lin_preds = predict(bike_nb, type = 'link'),
                         pearson = residuals(bike_nb, type = 'pearson'))

ggplot(bike_nb_df, aes(x = lin_preds, y = pearson)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  labs(x = "Linear Predictors",
       y = "Pearson Residuals",
       title = "Bike Rental Residual Plot")

ggplot(bike_nb_df, aes(sample = pearson)) +
  stat_qq() +
  stat_qq_line() +
  labs(x = "Theoretical Quantiles",
       y = "Sample Quantiles",
       title = "QQ Plot: Pearson Residuals")
```

This is clearly not a great model for the data but we'll run a GOF test anyway

```{r}
1 - pchisq(bike_nb$deviance, df = bike_nb$df.residual)
```

Since the p-value is zero we reject the null hypothesis and see that the model is not a good fit.
We can see why by looking at a plot of the predicted values and the actual values.

```{r}
bike_nb_pred <- exp(predict(bike_nb, type = "link"))

index <- 1:length(bike$cnt)

df1 <- data.frame(index = rep(index, 2),
                  incidents = c(bike$cnt, bike_nb_pred),
                  model = c(rep("Actual",length(bike$cnt)),
                            rep("NB Predictions", length(bike_nb_pred)))
)

ggplot(df1, aes(x = index, y = incidents, color = model)) +
  geom_line() +
  labs(x = "Index", y = "Bike Rentals", color = "Model")
```



\clearpage
# Comparison with Poisson

Since the Negative Binomial has an extra term for dispersion it is especially useful when the poisson model is overdispersed.

A brief example of a comparison with Poisson Regression can be obtained by using the data of ship accidents from Homework 5. (For ease of use we have added this to the package under the name `ship_accidents`)

```{r}
shipacc <- ship_accidents

shipacc_psn <- glm(incidents ~ ., data = shipacc, family = poisson)

shipacc_nb <- glm.nb(incidents ~ ., data = shipacc)


shipacc_psn_df <- data.frame(lin_preds = predict(shipacc_psn, type = 'link'),
                             pearson = residuals(shipacc_psn, type = 'pearson'),
                             model = "Poisson")

shipacc_nb_df <- data.frame(lin_preds = predict(shipacc_nb, type = 'link'),
                            pearson = residuals(shipacc_nb, type = 'pearson'),
                            model = "Negative Binomial")

df_combined <- rbind(shipacc_psn_df, shipacc_nb_df)

ggplot(df_combined, aes(x = lin_preds, y = pearson, color = model)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  labs(x = "Linear Predictors", 
       y = "Pearson Residuals", 
       title = "Ship Accidents Residual Plot")

ggplot(df_combined, aes(sample = pearson, color = model)) +
  stat_qq() +
  stat_qq_line() +
  labs(x = "Theoretical Quantiles",
       y = "Sample Quantiles",
       title = "QQ Plot: Pearson Residuals")
```

These are not great 
These can be slightly difficult to interpret, so we will conduct a goodness of fit test.

```{r, include = T}
shipacc_psn_pval <- 1 - pchisq(shipacc_psn$deviance, df = shipacc_psn$df.residual)
shipacc_nb_pval <- 1 - pchisq(shipacc_nb$deviance, df = shipacc_nb$df.residual)

# Poisson
shipacc_psn_pval
# Negative Binomial
shipacc_nb_pval
```

We have strong evidence that the natural Poisson model does not fit the data, however the Negative Binomial model performs much better here. The Poisson P-Value for the goodness of fit test is 0.0000002 and the P-Value for the Negative Binomial goodness of fit test is 0.007. We do have evidence that the Negative Binomial isn't a great fit for this data as well, but it performs considerably better than the Poisson model.

Plotting the predicted incidents on top of the actual incidents gives us a good idea of how the models match up!

```{r}
shipacc_psn_predictions <- exp(predict(shipacc_psn, type = "link"))
shipacc_nb_predictions <- exp(predict(shipacc_nb, type = "link"))

index <- 1:length(shipacc$incidents)

df1 <- data.frame(index = rep(index, 3),
                  incidents = c(shipacc$incidents, 
                                shipacc_psn_predictions, 
                                shipacc_nb_predictions),
                  model = c(rep("Actual", length(shipacc$incidents)), 
                            rep("psn_predictions", length(shipacc_psn_predictions)), 
                            rep("nb_predictions", length(shipacc_nb_predictions)))
)

ggplot(df1, aes(x = index, y = incidents, color = model)) +
  geom_point() +
  geom_line() +
  labs(x = "Index", y = "Incidents", color = "Model")
```


\clearpage
# Other models and adaptations

Just like the Poisson Regression had various adaptations, the Negative Binomial also has other methods dealing with certain cases.

1. **Zero-inflated Negative Binomial**: Used for count data that exhibits overdispersion and an excess of zeros.
2. **Rate-adjusted Negative Binomial**: Used when there are categories or specified time intervals to get a rate adapted model.

There are a lot more adaptations of the Negative binomial but these are some of the more popular options.